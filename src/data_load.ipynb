{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Data into Database:\n",
    "\n",
    "Before running the  code in this script, make sure that the tables are created in the database.\n",
    "\n",
    "If the tables in the database contain old data, refer to the SQL code file named firstcut_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from modules.connection_mod import connect, show_psycopg2_exception\n",
    "\n",
    "load_dotenv()\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the connection parameters:\n",
    "conn_params_dict = {\n",
    "    \"host\": os.getenv('PG_HOST'),\n",
    "    \"database\": os.getenv('DATABASE'),\n",
    "    \"user\": os.getenv('PG_USER'),\n",
    "    \"password\": os.getenv('PG_PASSWORD')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_many(conn, table_dataframe_pairs):\n",
    "    \"\"\"\n",
    "    This function will process the list of table and dataframe pairs and insert the data into the database.\n",
    "    \n",
    "    Args:\n",
    "        conn (_type_): connection object\n",
    "        table_dataframe_pairs (list of tuples): list of tuples containing dataframes and its corresponding table name\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    try:\n",
    "        for table, dataframe in table_dataframe_pairs:\n",
    "            tpls = [tuple(x) for x in dataframe.to_numpy()] #Creating a list of tuples from the dataframe values\n",
    "            \n",
    "            cols = ','.join(list(dataframe.columns)) #dataframe columns with Comma-separated\n",
    "            \n",
    "            # Generate the placeholders for values dynamically based on the number of columns in each dataframe\n",
    "            placeholders = ','.join(['%s'] * len(dataframe.columns))\n",
    "            \n",
    "            # SQL query to execute\n",
    "            sql = f\"INSERT INTO {table} ({cols}) VALUES ({placeholders})\"\n",
    "            \n",
    "            cursor.executemany(sql, tpls)\n",
    "        \n",
    "        conn.commit()\n",
    "        logger.info('Data inserted successfully using the execute_many() function')\n",
    "        print(\"Data inserted successfully for tables\")\n",
    "    except (Exception, psycopg2.DatabaseError) as err:\n",
    "        show_psycopg2_exception(err) #pass exception to function\n",
    "    finally:\n",
    "        cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "conn = connect(conn_params_dict)\n",
    "conn.autocommit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "patient = pd.read_csv('../COUNTRIES/USA/data/processed/MLI_Dec_1_2024-01-31_14-01.csv', index_col=False)\n",
    "condition = pd.read_csv('../COUNTRIES/USA/data/processed/MLI_Dec_2_2024-01-31_14-01.csv', index_col=False)\n",
    "microscopy = pd.read_csv('../COUNTRIES/USA/data/processed/MLI_Dec_3_2024-01-31_14-01.csv', index_col=False)\n",
    "culture = pd.read_csv('../COUNTRIES/USA/data/processed/MLI_Dec_4_2024-01-31_14-01.csv', index_col=False)\n",
    "specimen = pd.read_csv('../COUNTRIES/USA/data/processed/MLI_Dec_5_2024-01-31_14-01.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of tuples with the dataframes to upload\n",
    "table_dataframe_pairs = [\n",
    "    ('staging.patient_leo', patient),\n",
    "    ('staging.condition_temp_leo', condition),\n",
    "    ('staging.microscopy_leo', microscopy),\n",
    "    ('staging.culture_leo', culture),\n",
    "    ('staging.specimen_leo', specimen)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_many(conn, table_dataframe_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regimen upload failed. I think this is because it is expecting the drugs column to be an array.\n",
    "# I will have to do something similar to what I did with the condition table.\n",
    "\n",
    "# regimen_upload['drugs'] = regimen_upload['drugs'].str.replace(r'{|}', '', regex=True)\n",
    "# regimen_upload.to_csv('regimen_upload_temp3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regimen_upload_temp = pd.read_csv('Mexico/data/CB_data_file_2/regimen_upload_temp2.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treatment upload had to be done using the Import/Export function in PgAdmin.\n",
    "# For some reason it was failing because it wanted to status_code column to be present.\n",
    "\n",
    "# Condition and regimen need to be uploaded into temp tables because it is easier to transform them in PgAdmin\n",
    "# regimen_upload_temp upload had to be done using the Import/Export function in PgAdmin.\n",
    "# execute_many(conn, condition_upload, 'condition_temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Patient table data\n",
    "\n",
    "* This portion might not always be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "patient_july = pd.read_csv('../countries/Georgia/data/processed/Georgia_patient_july.csv', index_col=False)\n",
    "patient_june = pd.read_csv('../countries/Georgia/data/processed/Georgia_patient_june.csv', index_col=False)\n",
    "patient_august = pd.read_csv('../countries/Georgia/data/processed/Georgia_patient_august.csv', index_col=False)\n",
    "patient_september = pd.read_csv('../countries/Georgia/data/processed/Georgia_patient_september.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the execute_many method for tables:\n",
    "# execute_many(conn, patient_july, 'patient')\n",
    "# execute_many(conn, patient_june, 'patient')\n",
    "# execute_many(conn, patient_august, 'patient')\n",
    "# execute_many(conn, patient_september, 'patient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NTM-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
